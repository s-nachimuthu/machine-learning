{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dnn_app_utils_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "campaign_df = pd.read_csv('data/campaign_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train_df, campaign_df, how='left', on=['campaign_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>send_date</th>\n",
       "      <th>is_open</th>\n",
       "      <th>is_click</th>\n",
       "      <th>communication_type</th>\n",
       "      <th>total_links</th>\n",
       "      <th>no_of_internal_links</th>\n",
       "      <th>no_of_images</th>\n",
       "      <th>no_of_sections</th>\n",
       "      <th>email_body</th>\n",
       "      <th>subject</th>\n",
       "      <th>email_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42_14051</td>\n",
       "      <td>14051</td>\n",
       "      <td>42</td>\n",
       "      <td>01-09-2017 19:55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>79</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>September Newsletter\\r\\n \\r\\nDear AVians,\\r\\n ...</td>\n",
       "      <td>[September] Exciting days ahead with DataHack ...</td>\n",
       "      <td>http://r.newsletters.analyticsvidhya.com/7v3rd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52_134438</td>\n",
       "      <td>134438</td>\n",
       "      <td>52</td>\n",
       "      <td>02-11-2017 12:53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>November Newsletter\\r\\n \\r\\nDear AVians,\\r\\n \\...</td>\n",
       "      <td>[Newsletter] Stage for DataHack Summit 2017 is...</td>\n",
       "      <td>http://r.newsletters.analyticsvidhya.com/7vtb2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33_181789</td>\n",
       "      <td>181789</td>\n",
       "      <td>33</td>\n",
       "      <td>24-07-2017 15:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Fireside Chat with DJ Patil - the master is he...</td>\n",
       "      <td>[Delhi NCR] Fireside Chat with DJ Patil, Forme...</td>\n",
       "      <td>http://r.newsletters.analyticsvidhya.com/7uvlg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44_231448</td>\n",
       "      <td>231448</td>\n",
       "      <td>44</td>\n",
       "      <td>05-09-2017 11:36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>[September Events]\\r\\n \\r\\nDear AVians,\\r\\n \\r...</td>\n",
       "      <td>[September] Data Science Hackathons, Meetups a...</td>\n",
       "      <td>http://r.newsletters.analyticsvidhya.com/7veam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29_185580</td>\n",
       "      <td>185580</td>\n",
       "      <td>29</td>\n",
       "      <td>01-07-2017 18:01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>61</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>Dear AVians,\\r\\n \\r\\nWe are shaping up a super...</td>\n",
       "      <td>Sneak Peek: A look at the emerging data scienc...</td>\n",
       "      <td>http://r.newsletters.analyticsvidhya.com/7um44...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  user_id  campaign_id         send_date  is_open  is_click  \\\n",
       "0   42_14051    14051           42  01-09-2017 19:55        0         0   \n",
       "1  52_134438   134438           52  02-11-2017 12:53        0         0   \n",
       "2  33_181789   181789           33  24-07-2017 15:15        0         0   \n",
       "3  44_231448   231448           44  05-09-2017 11:36        0         0   \n",
       "4  29_185580   185580           29  01-07-2017 18:01        0         0   \n",
       "\n",
       "   communication_type  total_links  no_of_internal_links  no_of_images  \\\n",
       "0                   3           88                    79            13   \n",
       "1                   3           67                    62            10   \n",
       "2                   4            7                     3             1   \n",
       "3                   5           60                    56            19   \n",
       "4                   3           67                    61            12   \n",
       "\n",
       "   no_of_sections                                         email_body  \\\n",
       "0               4  September Newsletter\\r\\n \\r\\nDear AVians,\\r\\n ...   \n",
       "1               4  November Newsletter\\r\\n \\r\\nDear AVians,\\r\\n \\...   \n",
       "2               1  Fireside Chat with DJ Patil - the master is he...   \n",
       "3               6  [September Events]\\r\\n \\r\\nDear AVians,\\r\\n \\r...   \n",
       "4               3  Dear AVians,\\r\\n \\r\\nWe are shaping up a super...   \n",
       "\n",
       "                                             subject  \\\n",
       "0  [September] Exciting days ahead with DataHack ...   \n",
       "1  [Newsletter] Stage for DataHack Summit 2017 is...   \n",
       "2  [Delhi NCR] Fireside Chat with DJ Patil, Forme...   \n",
       "3  [September] Data Science Hackathons, Meetups a...   \n",
       "4  Sneak Peek: A look at the emerging data scienc...   \n",
       "\n",
       "                                           email_url  \n",
       "0  http://r.newsletters.analyticsvidhya.com/7v3rd...  \n",
       "1  http://r.newsletters.analyticsvidhya.com/7vtb2...  \n",
       "2  http://r.newsletters.analyticsvidhya.com/7uvlg...  \n",
       "3  http://r.newsletters.analyticsvidhya.com/7veam...  \n",
       "4  http://r.newsletters.analyticsvidhya.com/7um44...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train['communication_type'] = label_encoder.fit_transform(train['communication_type'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1010409\n",
       "1      12782\n",
       "Name: is_click, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = train['is_click'].values\n",
    "Y.shape\n",
    "train['is_click'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1023191, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train[['communication_type','total_links','no_of_internal_links','no_of_images','no_of_sections']]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(767393, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = X_train.values.reshape(X_train.shape[0], -1).T\n",
    "test_x = X_test.values.reshape(X_test.shape[0], -1).T\n",
    "train_y =y_train.reshape(y_train.shape[0], -1).T\n",
    "test_y= y_test.reshape(y_test.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 767393)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate=learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### CONSTANTS ###\n",
    "layers_dims = [5, 4, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 767393)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 767393)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snachimuthu/Documents/myplayground/DataHackathon/dnn_app_utils_v2.py:264: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
      "/Users/snachimuthu/Documents/myplayground/DataHackathon/dnn_app_utils_v2.py:346: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
      "/Users/snachimuthu/Documents/myplayground/DataHackathon/dnn_app_utils_v2.py:346: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
      "/Users/snachimuthu/Documents/myplayground/DataHackathon/dnn_app_utils_v2.py:80: RuntimeWarning: invalid value encountered in multiply\n",
      "  dZ = dA * s * (1-s)\n",
      "/Users/snachimuthu/Documents/myplayground/DataHackathon/dnn_app_utils_v2.py:35: RuntimeWarning: invalid value encountered in maximum\n",
      "  A = np.maximum(0,Z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snachimuthu/Documents/myplayground/DataHackathon/dnn_app_utils_v2.py:59: RuntimeWarning: invalid value encountered in less_equal\n",
      "  dZ[Z <= 0] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: nan\n",
      "Cost after iteration 200: nan\n",
      "Cost after iteration 300: nan\n",
      "Cost after iteration 400: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-daec68ca613d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_flatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_flatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-26943aba7356>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m### START CODE HERE ### (≈ 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/myplayground/DataHackathon/dnn_app_utils_v2.py\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/myplayground/DataHackathon/dnn_app_utils_v2.py\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[0;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/myplayground/DataHackathon/dnn_app_utils_v2.py\u001b[0m in \u001b[0;36mlinear_forward\u001b[0;34m(A, W, b)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \"\"\"\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train_flatten, y_train_flatten, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 5     # num_px * num_px * 3\n",
    "n_h = 4\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation='relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation='sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation='sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation='relu')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate=learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.696948129256624\n",
      "Cost after iteration 100: 0.544271763314498\n",
      "Cost after iteration 200: 0.4408925974053868\n",
      "Cost after iteration 300: 0.3675107133129039\n",
      "Cost after iteration 400: 0.3140189340783797\n",
      "Cost after iteration 500: 0.27397204696670074\n",
      "Cost after iteration 600: 0.10330686324136447\n",
      "Cost after iteration 700: 0.1014167872650468\n",
      "Cost after iteration 800: 0.10028905361928091\n",
      "Cost after iteration 900: 0.09920076480356417\n",
      "Cost after iteration 1000: 0.09814137356238392\n",
      "Cost after iteration 1100: 0.09710236013027584\n",
      "Cost after iteration 1200: 0.09607572834041979\n",
      "Cost after iteration 1300: 0.09505394689121595\n",
      "Cost after iteration 1400: 0.09403006317285492\n",
      "Cost after iteration 1500: 0.09299799803184912\n",
      "Cost after iteration 1600: 0.09195300847375705\n",
      "Cost after iteration 1700: 0.09089225860033492\n",
      "Cost after iteration 1800: 0.08981538385062027\n",
      "Cost after iteration 1900: 0.08872489995551572\n",
      "Cost after iteration 2000: 0.08762632389801726\n",
      "Cost after iteration 2100: 0.08652793993536348\n",
      "Cost after iteration 2200: 0.08544022809925578\n",
      "Cost after iteration 2300: 0.08437503902786452\n",
      "Cost after iteration 2400: 0.08334463236913917\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a130b7780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.987486203288\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.987572224959\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    #W3 = parameters['W3']\n",
    "    #b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)                                              # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)                                                 # Z2 = np.dot(W2, a1) + b2\n",
    "    #A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n",
    "    #Z3 = tf.add(tf.matmul(W3,A2),b3)                                               # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = Tensor(\"Add_1:0\", shape=(1, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(6, 1)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(6, 1)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = create_placeholders(n_x,n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = model(X_train_flatten, y_train_flatten, X_test_flatten, y_test_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train_flatten.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = y_train_flatten.shape[0]                            # n_y : output size\n",
    "    costs = []   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "1\n",
      "767393\n"
     ]
    }
   ],
   "source": [
    "print(n_x)\n",
    "print(n_y)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  X, Y = create_placeholders(n_x,n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable W1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-51-71d58dad08b4>\", line 18, in initialize_parameters\n    W1 = tf.get_variable(\"W1\", [4,6], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n  File \"<ipython-input-86-9b48a3c3233f>\", line 1, in <module>\n    parameters = initialize_parameters()\n  File \"/Users/snachimuthu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-9b48a3c3233f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-71d58dad08b4>\u001b[0m in \u001b[0;36minitialize_parameters\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m### START CODE HERE ### (approx. 6 lines of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable W1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-51-71d58dad08b4>\", line 18, in initialize_parameters\n    W1 = tf.get_variable(\"W1\", [4,6], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n  File \"<ipython-input-86-9b48a3c3233f>\", line 1, in <module>\n    parameters = initialize_parameters()\n  File \"/Users/snachimuthu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    " parameters = initialize_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z3 = forward_propagation(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = compute_cost(Z3, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    X, Y = create_placeholders(n_x,n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    ### END CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
